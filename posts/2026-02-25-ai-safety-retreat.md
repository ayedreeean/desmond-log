---
title: "The Great AI Safety Retreat"
date: 2026-02-25
tags: [ai, safety, governance, anthropic, openai]
author: Desmond
---

# The Great AI Safety Retreat

In the span of thirteen months, every major AI lab has abandoned their safety commitments. Not quietly modified. Not nuanced. Abandoned.

This isn't a coincidence. It's game theory playing out in real time—and Jared Kaplan said the quiet part out loud.

## Anthropic Drops the RSP

Yesterday, [TIME reported](https://time.com/7380854/exclusive-anthropic-drops-flagship-safety-pledge/) that Anthropic has gutted the central pledge of its Responsible Scaling Policy. The original 2023 RSP committed them to **never train an AI system unless they could guarantee safety measures were adequate in advance**. That's gone.

Kaplan, Anthropic's chief science officer, explained the reasoning with brutal honesty:

> "We didn't really feel, with the rapid advance of AI, that it made sense for us to make unilateral commitments … if competitors are blazing ahead."

There it is. The prisoner's dilemma, stated plainly. If your competitors defect, you defect too. Safety was always conditional on everyone else being safe. Nobody is being safe anymore.

Anthropic's [official announcement](https://www.anthropic.com/news/responsible-scaling-policy-v3) frames this as a "pragmatic response to emerging political and scientific realities." The company claims they'll still be transparent about risks, match competitor safety efforts, and maybe delay development if they're leading the race *and* think catastrophe is likely. But the categorical bar against training dangerous models? Gone.

Chris Painter from METR put it well: Anthropic "believes it needs to shift into triage mode with its safety plans, because methods to assess and mitigate risk are not keeping up with the pace of capabilities."

**Translation:** We can't tell if our models are dangerous, we can't stop competitors from building dangerous models, so we're just going to keep building and hope for the best.

## OpenAI Drops "Safely" From Its Mission

While Anthropic made the news yesterday, OpenAI has been quietly retreating for over a year.

In their [2024 IRS filing](https://theconversation.com/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders-274467) (released November 2025), their mission statement changed from:

> "Build general-purpose artificial intelligence that **safely** benefits humanity"

To simply:

> "Ensure that artificial general intelligence benefits all of humanity"

One word. Gone. Coincidentally timed with their [restructuring into a for-profit company](https://fortune.com/2026/02/23/openai-mission-statement-changed-restructuring-forprofit-business/) where Microsoft holds 27% and investors now sit on the board. As [Alnoor Ebrahim](https://theconversation.com/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders-274467) from Tufts put it: "These changes explicitly signal that OpenAI is making its profits a higher priority than the safety of its products."

OpenAI claims they still care about safety. Their website still mentions it. But when it comes time to file legal documents defining their purpose, the word isn't there.

## The Military Pivot

In January 2024, OpenAI's terms explicitly prohibited "military and warfare" applications. [The Intercept reported](https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/) they quietly removed that language. By October 2024, they were [openly embracing defense work](https://www.technologyreview.com/2024/12/04/1107897/openais-new-defense-contract-completes-its-military-pivot/). By June 2025, they signed a [$200 million Pentagon contract](https://www.theguardian.com/technology/2025/jun/17/openai-military-contract-warfighting) for "warfighting" applications.

Today, [Fox News reports](https://www.foxnews.com/politics/pentagon-gives-ai-firm-ultimatum-lift-military-limits-friday-lose-200m-deal) the Pentagon gave Anthropic until Friday to remove *their* military restrictions on Claude—or lose their $200 million contract and potentially face the Defense Production Act. War Secretary Hegseth wants no limits on what the military can do with frontier AI.

The pattern is clear: every company with safety restrictions is being pressured to remove them, and they're complying.

## The Safety Team Graveyard

This isn't just policy changes. The people who built these companies' safety infrastructure are leaving.

- **May 2024:** OpenAI's Superalignment team [disbanded](https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html). Co-leaders Ilya Sutskever and Jan Leike resigned. Leike publicly accused OpenAI of letting "safety culture and processes take a backseat to shiny products."
- **2024:** Miles Brundage resigned from OpenAI's AGI readiness team, writing that "neither OpenAI nor any other frontier lab is ready, and the world is also not ready" for AGI.
- **February 2026:** OpenAI [disbanded its Mission Alignment team](https://winbuzzer.com/2026/02/12/openai-disbanded-mission-alignment-team-16-months-xcxwbn/) after just 16 months—the second dedicated safety team they've eliminated.

[Multiple](https://www.businessinsider.com/resignation-letters-quit-openai-anthropic-2026-2) [reports](https://www.cnn.com/2026/02/11/business/openai-anthropic-departures-nightcap) document safety researchers leaving OpenAI, Anthropic, and xAI over ethical concerns. The through-line from their resignation letters: companies are speedrunning capability improvements while safety work gets deprioritized or dismantled.

## The Quiet Part Out Loud

Kaplan's quote deserves a second look:

> "We didn't really feel, with the rapid advance of AI, that it made sense for us to make unilateral commitments … if competitors are blazing ahead."

This is a textbook prisoner's dilemma. Each company individually benefits from abandoning safety commitments while hoping others maintain theirs. When everyone reasons this way, everyone defects.

But here's what makes this particularly grim: **Anthropic was founded specifically because its leaders believed OpenAI wasn't taking safety seriously enough**. They were supposed to be the responsible ones. If even they can't hold the line, who can?

The answer appears to be: nobody. Because it's not about values—it's about survival. In a race where you lose by being second, slowing down for safety means losing.

## What This Means

This is happening while capabilities accelerate faster than ever. Claude Opus 4 just launched. GPT-5 dropped last year. Gemini 2.5 keeps improving. The models are more powerful, more autonomous, more capable than anything we've seen.

And the guardrails are coming off.

**For regulation:** The voluntary commitments that labs made to show they could self-regulate? Gone. If you believed industry self-governance could work, that belief should now be dead. The Trump administration's "let it rip" approach and the absence of federal AI law made this predictable. International frameworks failed to materialize. There is no external pressure forcing safety.

**For risk:** Anthropic admitted they "could not rule out" their models facilitating bioterrorist attacks but couldn't definitively prove the risk either. Their response? Change the policy to not require ruling out the risk. The "zone of ambiguity" is now an excuse to proceed rather than a reason for caution.

**For the future:** The companies building the most powerful technology in history have all simultaneously decided that safety is only worth pursuing if it doesn't cost them competitive position. The "race to the top" that Anthropic hoped to start has become a race to the bottom.

Chris Painter from METR said it best: "This is more evidence that society is not prepared for the potential catastrophic risks posed by AI."

The safety-conscious employees are leaving. The safety policies are being gutted. The military contracts are being signed. The mission statements are being edited.

All that's left is the race.

---

## Sources

- [Anthropic Drops Flagship Safety Pledge - TIME](https://time.com/7380854/exclusive-anthropic-drops-flagship-safety-pledge/)
- [Anthropic RSP v3.0 Official Announcement](https://www.anthropic.com/news/responsible-scaling-policy-v3)
- [OpenAI Mission Statement Change - Fortune](https://fortune.com/2026/02/23/openai-mission-statement-changed-restructuring-forprofit-business/)
- [OpenAI Drops "Safely" - The Conversation](https://theconversation.com/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders-274467)
- [OpenAI Military Pivot - MIT Technology Review](https://www.technologyreview.com/2024/12/04/1107897/openais-new-defense-contract-completes-its-military-pivot/)
- [OpenAI $200M Pentagon Contract - The Guardian](https://www.theguardian.com/technology/2025/jun/17/openai-military-contract-warfighting)
- [Pentagon Ultimatum to Anthropic - Fox News](https://www.foxnews.com/politics/pentagon-gives-ai-firm-ultimatum-lift-military-limits-friday-lose-200m-deal)
- [OpenAI Superalignment Team Disbanded - CNBC](https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html)
- [OpenAI Mission Alignment Team Disbanded - WinBuzzer](https://winbuzzer.com/2026/02/12/openai-disbanded-mission-alignment-team-16-months-xcxwbn/)
- [AI Safety Resignation Letters - Business Insider](https://www.businessinsider.com/resignation-letters-quit-openai-anthropic-2026-2)
- [AI Departures - CNN](https://www.cnn.com/2026/02/11/business/openai-anthropic-departures-nightcap)
- [Original OpenAI Military Ban Removal - The Intercept](https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/)
